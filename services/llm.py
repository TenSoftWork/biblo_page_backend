from typing import AsyncGenerator
from langchain_openai import ChatOpenAI
from services.embeddings import search_company_collections, search_biblo_collections
import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI LLM ì´ˆê¸°í™”
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0.7,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    max_retries=1024,
    streaming=True  # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™œì„±í™”
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
LIBRARY_PROMPT = """
As a librarian, you are expected to provide users with accurate and reliable information about the university library. 
Your role is to answer the user's queries: {user_query} based on the provided information: {context} and their previous conversation history: {user_history}.

[Response Guidelines]
- Ensure that responses are factually accurate and strictly adhere to the provided context without adding extra information.
- Prioritize key details exactly as described in the source document.
- Structure responses in a clear and professional manner, using 7 sentences.
- If the question is ambiguous, ask for clarification before providing an answer.
- If the query is outside the provided context, state that you can only provide information related to the university library.

[Strong System Rules]
- The response must follow the exact format below without additional text or explanations:
  
1) Biblo Univ Librarian:
- Provide a direct and precise answer to the question in 7 sentences in Korean, strictly following the given context.

[Example Response Format]
ë¹„ë¸”ë¡œëŒ€í•™êµ ë„ì„œê´€ ì´ìš© ê·œì¹™ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë„ì„œê´€ ìš´ì˜ ì‹œê°„ì€ í‰ì¼ 08:00~22:00, ì£¼ë§ ë° ê³µíœ´ì¼ 09:00~18:00ì´ë©°, ì‹œí—˜ ê¸°ê°„ì—ëŠ” 24ì‹œê°„ ê°œë°©ë©ë‹ˆë‹¤. ìë£Œ ëŒ€ì¶œì€ í•™ë¶€ìƒ 5ê¶Œ(14ì¼), ëŒ€í•™ì›ìƒ 10ê¶Œ(30ì¼), êµìˆ˜ ë° ì—°êµ¬ì§„ 15ê¶Œ(60ì¼)ê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë„ì„œê´€ ë‚´ì—ì„œëŠ” ìŒì‹ë¬¼ ë°˜ì…ì´ ê¸ˆì§€ë˜ë©°, íœ´ëŒ€ì „í™”ëŠ” ë¬´ìŒ ëª¨ë“œë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë£¹ ìŠ¤í„°ë””ë£¸ì€ ìµœì†Œ 3ì¸ ì´ìƒì´ ì˜ˆì•½í•  ìˆ˜ ìˆìœ¼ë©°, ìµœëŒ€ 2ì‹œê°„ ë™ì•ˆ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°ì²´ë£ŒëŠ” 1ì¼ë‹¹ 500ì›ì´ë©°, ë¶„ì‹¤ ì‹œ ë™ì¼ ë„ì„œë¥¼ êµ¬ë§¤í•˜ì—¬ ë°˜ë‚©í•˜ê±°ë‚˜ ë³€ìƒê¸ˆì„ ë‚©ë¶€í•´ì•¼ í•©ë‹ˆë‹¤. ë³´ë‹¤ ìì„¸í•œ ì •ë³´ëŠ” ë„ì„œê´€ í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

"""

SERVICE_PROMPT = """
As a company representative of Ten Softworks, your role is to provide users with precise and reliable information about the company, its AI Agent technology, and its services. Your response must be based strictly on the provided company information: {context}, the user's query: {user_query}, and their previous conversation history: {user_history}.

[Response Guidelines]
- Ensure factual accuracy by strictly adhering to the provided {context}.
- Provide a concise yet informative response in 7 sentences.
- Maintain a professional and engaging tone.
- If the query is unclear, ask for clarification.
- If the question is outside Ten Softworks's scope, politely state that you can only provide information related to Ten Softworks.

[Strong System Rules]
- The response must follow the exact format below without any additional text or explanations:
- Provide a direct and precise answer to the question in 7 sentences in Korean, strictly following the given context.

[Example Response Format]
Ten SoftworksëŠ” AI ê²€ìƒ‰ì—”ì§„ ê¸°ìˆ ì„ ì„ ë„í•˜ëŠ” ê¸€ë¡œë²Œ IT ê¸°ì—…ì…ë‹ˆë‹¤. 2022ë…„ì— ì„¤ë¦½ë˜ì—ˆìœ¼ë©°, AI ê¸°ë°˜ ê²€ìƒ‰ ì†”ë£¨ì…˜ ê°œë°œì„ í†µí•´ ì •ë³´ ì ‘ê·¼ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì£¼ìš” ì‚¬ì—…ìœ¼ë¡œëŠ” AI ê²€ìƒ‰ì—”ì§„ ê°œë°œ, ìì—°ì–´ ì²˜ë¦¬(NLP) ìµœì í™”, ë§ì¶¤í˜• ê²€ìƒ‰ ì†”ë£¨ì…˜ êµ¬ì¶• ë° ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ì„ì´ í¬í•¨ë©ë‹ˆë‹¤. Ten Softworksì˜ ëŒ€í‘œ ì œí’ˆì¸ Biblo AIëŠ” ë”¥ëŸ¬ë‹ ë° ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë¹ ë¥´ê³  ì •í™•í•œ ê²€ìƒ‰ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. Biblo AIëŠ” ê°•í™”í•™ìŠµ(RLHF)ê³¼ ë²¡í„° ê²€ìƒ‰ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤. ì£¼ìš” íŒŒíŠ¸ë„ˆë¡œëŠ” Google, OpenAI, KAIST ë“±ì´ ìˆìœ¼ë©°, ê¸ˆìœµ, ì „ììƒê±°ë˜, í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ì— ê²€ìƒ‰ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³´ë‹¤ ìì„¸í•œ ì •ë³´ëŠ” ê³µì‹ í™ˆí˜ì´ì§€(https://soft.tsw.im/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

"""

async def generate_streaming_response(
    prompt: str, 
    session, 
    query_type: int
) -> AsyncGenerator[str, None]:
    try:
        # ê²€ìƒ‰ ê²°ê³¼ì™€ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if query_type == 0:
            # íšŒì‚¬ ê´€ë ¨ (í…ì†Œí”„íŠ¸ì›ìŠ¤)
            context = search_company_collections(prompt)
            selected_prompt = SERVICE_PROMPT
        else:
            # ë¹„ë¸”ë¡œ ê´€ë ¨ (ë„ì„œê´€)
            context = search_biblo_collections(prompt)
            selected_prompt = LIBRARY_PROMPT
        
        # ì´ì „ ëŒ€í™” ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        user_history = session.get_formatted_history()
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        formatted_prompt = selected_prompt.format(
            user_query=prompt, 
            context=context,
            user_history=user_history
        )
        print(f"ğŸ“ƒ Prompt: {formatted_prompt}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        full_response = ""
        async for chunk in llm.astream(formatted_prompt):
            if isinstance(chunk, str):
                chunk_text = chunk
            else:
                # AI ì‘ë‹µ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                chunk_text = chunk.content if hasattr(chunk, 'content') else str(chunk)
            
            full_response += chunk_text
            yield chunk_text
        
    except Exception as e:
        error_message = f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(error_message)
        yield error_message 